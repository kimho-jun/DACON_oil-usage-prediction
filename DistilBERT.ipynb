{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22da5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import copy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "f143834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_token['token2'] = use_token['token'].apply(lambda x:preprocess_and_layer_embedding(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "id": "90abedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_number = use_token.loc[use_token['token2'][i].str.len() >= 2].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "id": "7d8a348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list = []\n",
    "for i in range(len(use_token)):\n",
    "    if len (use_token['token2'][i]) >= 2:\n",
    "        remove_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "id": "83d190e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.49</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.08</td>\n",
       "      <td>-0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198892</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198893</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198894</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198895</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198896</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>1.55</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198897 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4     5     6     7     8     9    ...   758  \\\n",
       "0      -0.33 -0.79 -0.14 -0.01 -0.41  0.24  0.25  0.30 -0.10 -0.04  ...  0.25   \n",
       "1       0.08  0.66 -0.56 -0.77 -0.21 -0.48  0.65 -0.31  0.90 -0.50  ...  0.57   \n",
       "2      -1.49  0.73  0.29 -0.09  1.05 -0.26  0.26 -0.48 -0.71 -0.07  ...  0.53   \n",
       "3       0.48 -0.91  0.05 -0.01  0.26 -0.27 -2.22 -0.61 -0.32 -0.19  ...  2.29   \n",
       "4      -0.41  0.64  0.25  0.46 -0.79  0.99  0.47 -0.50  0.34 -0.53  ...  0.54   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "198892 -0.17  1.23  0.01 -0.16 -0.35  1.24 -0.40 -0.63  1.03 -0.40  ... -0.10   \n",
       "198893 -0.51  0.14 -0.75 -0.17 -0.29  0.56  1.04  1.45 -0.06 -0.48  ...  0.88   \n",
       "198894  1.19  0.29 -0.39  0.43  1.45 -0.58  0.15 -0.35 -1.28 -0.11  ... -0.49   \n",
       "198895  0.47  0.81  0.05 -0.78 -0.38  0.80  0.91  1.24  0.13  0.82  ... -0.00   \n",
       "198896 -0.00 -0.59 -0.64  0.22  0.68  0.14  0.58 -0.35  1.55 -0.16  ...  0.38   \n",
       "\n",
       "         759   760   761   762   763   764   765   766   767  \n",
       "0      -0.18 -0.33 -0.23  0.13  0.09  0.27 -0.17 -0.11  0.64  \n",
       "1       0.21  0.36 -0.81 -0.28  0.77 -0.10  0.03  0.64  0.37  \n",
       "2       0.96 -0.04 -0.21  0.89  0.12 -0.11 -0.01  1.08 -0.38  \n",
       "3      -0.11 -1.31 -0.70  1.08  1.08  1.03 -0.15 -0.61  0.14  \n",
       "4       0.86 -0.33  0.48 -0.26 -0.29  0.43  0.40 -0.84 -0.28  \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "198892  0.59  0.15  0.53  0.04  0.66  0.34  0.28 -1.00 -0.92  \n",
       "198893 -0.67  0.54  0.11 -1.19 -0.28 -1.08  1.32  0.46 -0.40  \n",
       "198894 -0.77  0.59 -0.10 -0.45 -0.06  0.82  0.14 -0.76 -1.09  \n",
       "198895 -0.07 -0.11 -1.04 -0.09  0.41 -0.01 -0.91  0.09 -0.51  \n",
       "198896  0.49  0.15  0.29 -0.69 -0.96  0.86 -0.09  0.22  0.76  \n",
       "\n",
       "[198897 rows x 768 columns]"
      ]
     },
     "execution_count": 1077,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "id": "67785aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_token =list(use_token['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71117b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from transformers import DistilBertTokenizer, DistilBertModel,DistilBertConfig\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased',output_hidden_states = True, output_attentions =True)\n",
    "\n",
    "# model = DistilBertModel.from_pretrained('distilbert-base-uncased',output_hidden_states =True)\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation) \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def preprocess_and_layer_embedding(tokens,idx):\n",
    "    \n",
    "    encode_tokens = tokenizer.encode(tokens,\n",
    "                                    add_special_tokens=False,\n",
    "                                    max_length=512,\n",
    "                                    truncation=True)\n",
    "#     return encode_tokens\n",
    "    embedding_tensor= []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "#         outputs = model(input_ids=torch.LongTensor(tokens).unsqueeze(0))\n",
    "        outputs = model(input_ids=torch.LongTensor(encode_tokens).unsqueeze(0))\n",
    "        hidden_states = outputs.hidden_states  \n",
    "        \n",
    "        layer_embedding = hidden_states[idx]\n",
    "        embedding_tensor.append(layer_embedding)\n",
    "    \n",
    "    embedding_tensor = embedding_tensor[0][0].view(-1,768)\n",
    "    embedding_vector = embedding_tensor.numpy()\n",
    "    \n",
    "    return embedding_vector\n",
    "\n",
    "# 원하는 layer에서 임베딩 벡터 출력하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f0c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator = str.maketrans('', '', string.punctuation)\n",
    "def get_token(text):\n",
    "#     my_text = copy.copy(text.translate(translator)) \n",
    "    my_text =copy.copy(text)\n",
    "    my_text = my_text.replace('br', '') # HTML 태그 제거\n",
    "    my_text = my_text.replace('\\t', '') # 띄어 쓰기 제거 \n",
    "    p = re.compile(r'<br\\s*/?>|[^A-Za-z]') # \n",
    "    my_text = p.sub(' ', my_text)\n",
    "    \n",
    "    tokens = tokenizer(my_text,\n",
    "                       truncation = True,\n",
    "                           return_tensors= 'pt',\n",
    "                           add_special_tokens = False,\n",
    "                           max_length=512\n",
    "                       )\n",
    "    \n",
    "    tokens_no_stopwords = [token for token in tokens['input_ids'][0] \n",
    "                           if (tokenizer.convert_ids_to_tokens(token.item()) not in stopwords)and len(tokenizer.convert_ids_to_tokens(token.item())) > 2] \n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokens_no_stopwords)\n",
    "    \n",
    "    last_tokens = []\n",
    "    for token in tokens:\n",
    "        if not token.startswith('#'):\n",
    "            last_tokens.append(token)\n",
    "        \n",
    "    return last_tokens\n",
    "    \n",
    "#     result = ' '.join(last_tokens)\n",
    "    \n",
    "#     return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "44fb5696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'apple']"
      ]
     },
     "execution_count": 903,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_token_by_distilbert('hi mt name is apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4d4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "def get_token_by_distilbert(text):\n",
    "    my_text = copy.copy(text.translate(translator)) \n",
    "    my_text = my_text.replace('br', '') # HTML 태그 제거\n",
    "    my_text = my_text.replace('\\t', '') # 띄어 쓰기 제거 \n",
    "    p = re.compile(r'<br\\s*/?>|[^A-Za-z]') # \n",
    "    my_text = p.sub(' ', my_text)\n",
    "    my_text = my_text.lower()\n",
    "    \n",
    "#     tree_tokenizer = TreebankWordTokenizer()\n",
    "    tokens = tokenizer(my_text,\n",
    "                       return_tensors = 'pt',\n",
    "                       add_special_tokens=False,\n",
    "                       max_length=512,\n",
    "                       truncation=True\n",
    "    )\n",
    "    \n",
    "    f_tokens = [token for token in tokens['input_ids'][0]\n",
    "                   if tokenizer.convert_ids_to_tokens(token.item()) not in stopwords and len(tokenizer.convert_ids_to_tokens(token.item())) > 2]\n",
    "    \n",
    "    temp_words = tokenizer.convert_ids_to_tokens(f_tokens)\n",
    "\n",
    "    pos_words = pos_tag(temp_words) \n",
    "    pos_filter = []\n",
    "    for word, pos in pos_words:\n",
    "        if  (pos == 'NN') or (pos == 'JJ') or (pos == 'VB'):\n",
    "            # PRP 는 you 같은 단어 , VBP는 are같은 필요 없는 동사, RB는 r\n",
    "            pos_filter.append(word)\n",
    "            \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = []\n",
    "    for word in pos_filter:\n",
    "        lemmatization_words = lemmatizer.lemmatize(word)\n",
    "        lemma_words.append(lemmatization_words)\n",
    "        \n",
    "    return lemma_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999ed8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subwords(tokens):\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if not token.startswith('#'):\n",
    "            words.append(token)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b9bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_row_remove_by_frequency(tokens):\n",
    "    words = []\n",
    "    row_count = Counter(tokens)\n",
    "    for key,value in row_count.items():\n",
    "        if row_count[key] > len(tokens) * 0.01:\n",
    "            words.append(key)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "65440eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tree_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "c2b685cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "c7feb0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr,x_val, y_tr,y_val = train_test_split(df['text'], df['target'], test_size =0.5, stratify=df['target'], random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "2a4a69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = copy.copy(pd.DataFrame(x_tr))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef073f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "098b50e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['token'] = tree_df['text'].apply(lambda x: get_token_by_distilbert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "7526adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['token2'] = tree_df['token'].apply(lambda x: remove_subwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "144436c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = []\n",
    "for i in range(len(tree_df)):\n",
    "    temp1+=tree_df['token2'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "77a8499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(temp1)\n",
    "\n",
    "freq_90_words = []\n",
    "for key, value in word_count.items():\n",
    "    if word_count[key] > 90:\n",
    "        freq_90_words.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "2fcbea7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_90_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "2006e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['token3'] = tree_df['token2'].apply(lambda x: ' '.join(freq_filter_words(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "id": "d7afa527",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= 0\n",
    "for i in range(len(tree_df)):\n",
    "    a+=len(tree_df['token2'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "c93ec328",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for i in range(len(tree_df)):\n",
    "    all_tokens+=tree_df['token2'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "id": "80a84b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['freq_token'] = tree_df['token2'].apply(lambda x: per_row_remove_by_frequency(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "c877e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for i in range(len(tree_df)):\n",
    "    tokens+=tree_df['freq_token'][i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "id": "1f9499a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315618"
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d817197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "id": "e9c6c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['input_token'] = tree_df['freq_token'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "id": "fe625cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>input_token</th>\n",
       "      <th>freq_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where can I get documentation about the X-Serv...</td>\n",
       "      <td>[get, documentation, ##ern, interested, docume...</td>\n",
       "      <td>[get, documentation, interested, documentation...</td>\n",
       "      <td>get documentation interested welcome rainer</td>\n",
       "      <td>[get, documentation, interested, welcome, rainer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nNot any more the rules don't say that.  ...</td>\n",
       "      <td>[##s, dumb, argument]</td>\n",
       "      <td>[dumb, argument]</td>\n",
       "      <td>dumb argument</td>\n",
       "      <td>[dumb, argument]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Netters\\nI want to send EMG-signals from ...</td>\n",
       "      <td>[dear, net, send, person, computer, signal, kh...</td>\n",
       "      <td>[dear, net, send, person, computer, signal, kh...</td>\n",
       "      <td>dear net send person computer signal khz wide ...</td>\n",
       "      <td>[dear, net, send, person, computer, signal, kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes -- my error -- you will need the DIFF betw...</td>\n",
       "      <td>[error, ##ff, standard, console, console, thin...</td>\n",
       "      <td>[error, standard, console, console, think, sig...</td>\n",
       "      <td>error standard console think signature file ma...</td>\n",
       "      <td>[error, standard, console, think, signature, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I do not think it is at all unlikely that Clin...</td>\n",
       "      <td>[think, unlikely, clinton, policy, display, pi...</td>\n",
       "      <td>[think, unlikely, clinton, policy, display, pi...</td>\n",
       "      <td>think unlikely clinton policy display piece gu...</td>\n",
       "      <td>[think, unlikely, clinton, policy, display, pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9064</th>\n",
       "      <td>Greetings all.\\n\\tAccording to a FAQ I read, o...</td>\n",
       "      <td>[##s, ##q, july, joshua, jensen, article, bit,...</td>\n",
       "      <td>[july, joshua, jensen, article, bit, manipulat...</td>\n",
       "      <td>july joshua jensen article bit manipulation pe...</td>\n",
       "      <td>[july, joshua, jensen, article, bit, manipulat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9065</th>\n",
       "      <td>What can be done, short of circumcision, for a...</td>\n",
       "      <td>[short, ##rc, adult, male, fore, ##ct]</td>\n",
       "      <td>[short, adult, male, fore]</td>\n",
       "      <td>short adult male fore</td>\n",
       "      <td>[short, adult, male, fore]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9066</th>\n",
       "      <td>The Branch Davidians were not violent and were...</td>\n",
       "      <td>[branch, david, violent, start, violence, ##d,...</td>\n",
       "      <td>[branch, david, violent, start, violence, comp...</td>\n",
       "      <td>branch david violent start violence compound f...</td>\n",
       "      <td>[branch, david, violent, start, violence, comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>When I changed from SVR4 to SVR4.2 on my Intel...</td>\n",
       "      <td>[##r, intel, box, etc, longer, work, bomb, mes...</td>\n",
       "      <td>[intel, box, etc, longer, work, bomb, message,...</td>\n",
       "      <td>intel box etc longer work bomb message error o...</td>\n",
       "      <td>[intel, box, etc, longer, work, bomb, message,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9068</th>\n",
       "      <td>-*----\\n\\nMing-zhou Liu's main problem is that...</td>\n",
       "      <td>[##zhou, liu, main, problem, inc, physician, p...</td>\n",
       "      <td>[liu, main, problem, inc, physician, physician...</td>\n",
       "      <td>liu main problem inc physician diagnosed disea...</td>\n",
       "      <td>[liu, main, problem, inc, physician, diagnosed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9069 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Where can I get documentation about the X-Serv...   \n",
       "1     \\n\\n\\nNot any more the rules don't say that.  ...   \n",
       "2     Dear Netters\\nI want to send EMG-signals from ...   \n",
       "3     Yes -- my error -- you will need the DIFF betw...   \n",
       "4     I do not think it is at all unlikely that Clin...   \n",
       "...                                                 ...   \n",
       "9064  Greetings all.\\n\\tAccording to a FAQ I read, o...   \n",
       "9065  What can be done, short of circumcision, for a...   \n",
       "9066  The Branch Davidians were not violent and were...   \n",
       "9067  When I changed from SVR4 to SVR4.2 on my Intel...   \n",
       "9068  -*----\\n\\nMing-zhou Liu's main problem is that...   \n",
       "\n",
       "                                                  token  \\\n",
       "0     [get, documentation, ##ern, interested, docume...   \n",
       "1                                 [##s, dumb, argument]   \n",
       "2     [dear, net, send, person, computer, signal, kh...   \n",
       "3     [error, ##ff, standard, console, console, thin...   \n",
       "4     [think, unlikely, clinton, policy, display, pi...   \n",
       "...                                                 ...   \n",
       "9064  [##s, ##q, july, joshua, jensen, article, bit,...   \n",
       "9065             [short, ##rc, adult, male, fore, ##ct]   \n",
       "9066  [branch, david, violent, start, violence, ##d,...   \n",
       "9067  [##r, intel, box, etc, longer, work, bomb, mes...   \n",
       "9068  [##zhou, liu, main, problem, inc, physician, p...   \n",
       "\n",
       "                                                 token2  \\\n",
       "0     [get, documentation, interested, documentation...   \n",
       "1                                      [dumb, argument]   \n",
       "2     [dear, net, send, person, computer, signal, kh...   \n",
       "3     [error, standard, console, console, think, sig...   \n",
       "4     [think, unlikely, clinton, policy, display, pi...   \n",
       "...                                                 ...   \n",
       "9064  [july, joshua, jensen, article, bit, manipulat...   \n",
       "9065                         [short, adult, male, fore]   \n",
       "9066  [branch, david, violent, start, violence, comp...   \n",
       "9067  [intel, box, etc, longer, work, bomb, message,...   \n",
       "9068  [liu, main, problem, inc, physician, physician...   \n",
       "\n",
       "                                            input_token  \\\n",
       "0           get documentation interested welcome rainer   \n",
       "1                                         dumb argument   \n",
       "2     dear net send person computer signal khz wide ...   \n",
       "3     error standard console think signature file ma...   \n",
       "4     think unlikely clinton policy display piece gu...   \n",
       "...                                                 ...   \n",
       "9064  july joshua jensen article bit manipulation pe...   \n",
       "9065                              short adult male fore   \n",
       "9066  branch david violent start violence compound f...   \n",
       "9067  intel box etc longer work bomb message error o...   \n",
       "9068  liu main problem inc physician diagnosed disea...   \n",
       "\n",
       "                                             freq_token  \n",
       "0     [get, documentation, interested, welcome, rainer]  \n",
       "1                                      [dumb, argument]  \n",
       "2     [dear, net, send, person, computer, signal, kh...  \n",
       "3     [error, standard, console, think, signature, f...  \n",
       "4     [think, unlikely, clinton, policy, display, pi...  \n",
       "...                                                 ...  \n",
       "9064  [july, joshua, jensen, article, bit, manipulat...  \n",
       "9065                         [short, adult, male, fore]  \n",
       "9066  [branch, david, violent, start, violence, comp...  \n",
       "9067  [intel, box, etc, longer, work, bomb, message,...  \n",
       "9068  [liu, main, problem, inc, physician, diagnosed...  \n",
       "\n",
       "[9069 rows x 5 columns]"
      ]
     },
     "execution_count": 1099,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "id": "1c9bf7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>token</th>\n",
       "      <th>token2</th>\n",
       "      <th>input_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where can I get documentation about the X-Serv...</td>\n",
       "      <td>[get, documentation, ##ern, interested, docume...</td>\n",
       "      <td>[get, documentation, interested, documentation...</td>\n",
       "      <td>get documentation interested documentation wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nNot any more the rules don't say that.  ...</td>\n",
       "      <td>[##s, dumb, argument]</td>\n",
       "      <td>[dumb, argument]</td>\n",
       "      <td>dumb argument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Netters\\nI want to send EMG-signals from ...</td>\n",
       "      <td>[dear, net, send, person, computer, signal, kh...</td>\n",
       "      <td>[dear, net, send, person, computer, signal, kh...</td>\n",
       "      <td>dear net send person computer signal khz wide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes -- my error -- you will need the DIFF betw...</td>\n",
       "      <td>[error, ##ff, standard, console, console, thin...</td>\n",
       "      <td>[error, standard, console, console, think, sig...</td>\n",
       "      <td>error standard console console think signature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I do not think it is at all unlikely that Clin...</td>\n",
       "      <td>[think, unlikely, clinton, policy, display, pi...</td>\n",
       "      <td>[think, unlikely, clinton, policy, display, pi...</td>\n",
       "      <td>think unlikely clinton policy display piece gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>Greetings all.\\n\\tAccording to a FAQ I read, o...</td>\n",
       "      <td>[##s, ##q, july, joshua, jensen, article, bit,...</td>\n",
       "      <td>[july, joshua, jensen, article, bit, manipulat...</td>\n",
       "      <td>july joshua jensen article bit manipulation pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9084</th>\n",
       "      <td>What can be done, short of circumcision, for a...</td>\n",
       "      <td>[short, ##rc, adult, male, fore, ##ct]</td>\n",
       "      <td>[short, adult, male, fore]</td>\n",
       "      <td>short adult male fore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9085</th>\n",
       "      <td>The Branch Davidians were not violent and were...</td>\n",
       "      <td>[branch, david, violent, start, violence, ##d,...</td>\n",
       "      <td>[branch, david, violent, start, violence, comp...</td>\n",
       "      <td>branch david violent start violence compound f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9086</th>\n",
       "      <td>When I changed from SVR4 to SVR4.2 on my Intel...</td>\n",
       "      <td>[##r, intel, box, etc, longer, work, bomb, mes...</td>\n",
       "      <td>[intel, box, etc, longer, work, bomb, message,...</td>\n",
       "      <td>intel box etc longer work bomb message error o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>-*----\\n\\nMing-zhou Liu's main problem is that...</td>\n",
       "      <td>[##zhou, liu, main, problem, inc, physician, p...</td>\n",
       "      <td>[liu, main, problem, inc, physician, physician...</td>\n",
       "      <td>liu main problem inc physician physician probl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9088 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     Where can I get documentation about the X-Serv...   \n",
       "1     \\n\\n\\nNot any more the rules don't say that.  ...   \n",
       "2     Dear Netters\\nI want to send EMG-signals from ...   \n",
       "3     Yes -- my error -- you will need the DIFF betw...   \n",
       "4     I do not think it is at all unlikely that Clin...   \n",
       "...                                                 ...   \n",
       "9083  Greetings all.\\n\\tAccording to a FAQ I read, o...   \n",
       "9084  What can be done, short of circumcision, for a...   \n",
       "9085  The Branch Davidians were not violent and were...   \n",
       "9086  When I changed from SVR4 to SVR4.2 on my Intel...   \n",
       "9087  -*----\\n\\nMing-zhou Liu's main problem is that...   \n",
       "\n",
       "                                                  token  \\\n",
       "0     [get, documentation, ##ern, interested, docume...   \n",
       "1                                 [##s, dumb, argument]   \n",
       "2     [dear, net, send, person, computer, signal, kh...   \n",
       "3     [error, ##ff, standard, console, console, thin...   \n",
       "4     [think, unlikely, clinton, policy, display, pi...   \n",
       "...                                                 ...   \n",
       "9083  [##s, ##q, july, joshua, jensen, article, bit,...   \n",
       "9084             [short, ##rc, adult, male, fore, ##ct]   \n",
       "9085  [branch, david, violent, start, violence, ##d,...   \n",
       "9086  [##r, intel, box, etc, longer, work, bomb, mes...   \n",
       "9087  [##zhou, liu, main, problem, inc, physician, p...   \n",
       "\n",
       "                                                 token2  \\\n",
       "0     [get, documentation, interested, documentation...   \n",
       "1                                      [dumb, argument]   \n",
       "2     [dear, net, send, person, computer, signal, kh...   \n",
       "3     [error, standard, console, console, think, sig...   \n",
       "4     [think, unlikely, clinton, policy, display, pi...   \n",
       "...                                                 ...   \n",
       "9083  [july, joshua, jensen, article, bit, manipulat...   \n",
       "9084                         [short, adult, male, fore]   \n",
       "9085  [branch, david, violent, start, violence, comp...   \n",
       "9086  [intel, box, etc, longer, work, bomb, message,...   \n",
       "9087  [liu, main, problem, inc, physician, physician...   \n",
       "\n",
       "                                            input_token  \n",
       "0     get documentation interested documentation wel...  \n",
       "1                                         dumb argument  \n",
       "2     dear net send person computer signal khz wide ...  \n",
       "3     error standard console console think signature...  \n",
       "4     think unlikely clinton policy display piece gu...  \n",
       "...                                                 ...  \n",
       "9083  july joshua jensen article bit manipulation pe...  \n",
       "9084                              short adult male fore  \n",
       "9085  branch david violent start violence compound f...  \n",
       "9086  intel box etc longer work bomb message error o...  \n",
       "9087  liu main problem inc physician physician probl...  \n",
       "\n",
       "[9088 rows x 4 columns]"
      ]
     },
     "execution_count": 961,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "b3a416d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_vec = TfidfVectorizer(max_features=500)\n",
    "# tfidf_matrix = tfidf_vec.fit_transform(tree_df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "7d71c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_word = list(tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "id": "5f23bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle(weight_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02133431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c29ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "ef4c196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(len(tree_df)):\n",
    "    a+=tree_df['token'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "170b0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "e16305df",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_90_words = []\n",
    "for word, value in word_count.items():\n",
    "    if word_count[word] > 90:\n",
    "        freq_90_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "e9ab7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_filter_words(tokens):\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token in freq_90_words:\n",
    "            words.append(token)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "9e14fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_df['tfidf_words'] = tree_df['token'].apply(lambda x: freq_filter_words(x))\n",
    "tree_df['tfidf_words'] = tree_df['token'].apply(lambda x: ' '.join(freq_filter_words(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "e9fe625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for i in range(len(tree_df)):\n",
    "    t += tree_df['tfidf_words'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "3b543740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275414"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "a6ba973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(max_features=250)\n",
    "\n",
    "tfidf_matrix = tfidf_vec.fit_transform(tree_df['tfidf_words'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "3222bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_words = list(tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "2ca0c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(last_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "e6579de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['last_words'] = tree_df['tfidf_words'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "1a3c86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_filter_words(tokens):\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        if token in last_words:\n",
    "            words.append(token)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9489085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "4f168f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['input_token'] = tree_df['last_words'].apply(lambda x: ' '.join(last_filter_words(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "d8931d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['temp'] = tree_df['input_token'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "6ad6e962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_token</th>\n",
       "      <th>toal_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get interested</td>\n",
       "      <td>[get, interested]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>send person computer person use possible send ...</td>\n",
       "      <td>[send, person, computer, person, use, possible...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>need standard file please anonymous please</td>\n",
       "      <td>[need, standard, file, please, anonymous, please]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think display gun war look get war white house...</td>\n",
       "      <td>[think, display, gun, war, look, get, war, whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>software disk new copy everything card card me...</td>\n",
       "      <td>[software, disk, new, copy, everything, card, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8750</th>\n",
       "      <td>time computer money anyone kind small real pro...</td>\n",
       "      <td>[time, computer, money, anyone, kind, small, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8751</th>\n",
       "      <td>article article article source code current em...</td>\n",
       "      <td>[article, article, article, source, code, curr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8752</th>\n",
       "      <td>start care clear big thing last day</td>\n",
       "      <td>[start, care, clear, big, thing, last, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8753</th>\n",
       "      <td>box work message cant open display cant find</td>\n",
       "      <td>[box, work, message, cant, open, display, cant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8754</th>\n",
       "      <td>problem problem idea similar first case help t...</td>\n",
       "      <td>[problem, problem, idea, similar, first, case,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8755 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            input_token  \\\n",
       "0                                        get interested   \n",
       "1     send person computer person use possible send ...   \n",
       "2            need standard file please anonymous please   \n",
       "3     think display gun war look get war white house...   \n",
       "4     software disk new copy everything card card me...   \n",
       "...                                                 ...   \n",
       "8750  time computer money anyone kind small real pro...   \n",
       "8751  article article article source code current em...   \n",
       "8752                start care clear big thing last day   \n",
       "8753       box work message cant open display cant find   \n",
       "8754  problem problem idea similar first case help t...   \n",
       "\n",
       "                                             toal_words  \n",
       "0                                     [get, interested]  \n",
       "1     [send, person, computer, person, use, possible...  \n",
       "2     [need, standard, file, please, anonymous, please]  \n",
       "3     [think, display, gun, war, look, get, war, whi...  \n",
       "4     [software, disk, new, copy, everything, card, ...  \n",
       "...                                                 ...  \n",
       "8750  [time, computer, money, anyone, kind, small, r...  \n",
       "8751  [article, article, article, source, code, curr...  \n",
       "8752        [start, care, clear, big, thing, last, day]  \n",
       "8753  [box, work, message, cant, open, display, cant...  \n",
       "8754  [problem, problem, idea, similar, first, case,...  \n",
       "\n",
       "[8755 rows x 2 columns]"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_df[['input_token', 'toal_words' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "b1646a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_words = []\n",
    "for i in range(len(tree_df)):\n",
    "    temp_words += tree_df['temp'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "0a83b922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149898"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "11d4cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_value = list(tree_df[tree_df['input_token'] == ''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "fc619a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df.drop(index=no_value, inplace = True)\n",
    "tree_df.reset_index(drop = True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff39cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['toal_words'] = tree_df['input_token'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in range(len(tree_df)):\n",
    "    words += tree_df['toal_words'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "50dfd062",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df.to_csv('prerprocess_tree_df(2).csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd6bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c106f86a",
   "metadata": {},
   "source": [
    "# 여기까지 전처리 완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "4ed36c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198847"
      ]
     },
     "execution_count": 984,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "id": "eaae20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_token = pd.DataFrame(tokens, columns = ['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "c22319cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_token.to_csv('use_token.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "3a70b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_val_list= list(tree_news20_df[tree_news20_df['tree_token'] == ''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "165bb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_news20_df.drop(index=no_val_list,inplace=True)  # 제거 18295개 남음\n",
    "tree_news20_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "6bfb0382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tree_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>sure bashers pens fans pretty confused lack ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>market highperformance video card supports ves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>finally said dream mediterranean new area grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>think scsi card dma transfers disks scsi card ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>old jasmine drive use new system understanding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18290</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>13</td>\n",
       "      <td>nyedacnsvaxuwecedu david nye neurology consult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18291</th>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>12</td>\n",
       "      <td>isolated ground recepticles usually unusual co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18292</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "      <td>installed cpu clone motherboard tried mounting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18293</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>1</td>\n",
       "      <td>wouldnt require hypersphere space points speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18294</th>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>7</td>\n",
       "      <td>tip gary crum crumfcomccutahedu got phone pont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18295 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target  \\\n",
       "0      \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1      My brother is in the market for a high-perform...       3   \n",
       "2      \\n\\n\\n\\n\\tFinally you said what you dream abou...      17   \n",
       "3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3   \n",
       "4      1)    I have an old Jasmine drive which I cann...       4   \n",
       "...                                                  ...     ...   \n",
       "18290  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...      13   \n",
       "18291  \\nNot in isolated ground recepticles (usually ...      12   \n",
       "18292  I just installed a DX2-66 CPU in a clone mothe...       3   \n",
       "18293  \\nWouldn't this require a hyper-sphere.  In 3-...       1   \n",
       "18294  After a tip from Gary Crum (crum@fcom.cc.utah....       7   \n",
       "\n",
       "                                              tree_token  \n",
       "0      sure bashers pens fans pretty confused lack ki...  \n",
       "1      market highperformance video card supports ves...  \n",
       "2      finally said dream mediterranean new area grea...  \n",
       "3      think scsi card dma transfers disks scsi card ...  \n",
       "4      old jasmine drive use new system understanding...  \n",
       "...                                                  ...  \n",
       "18290  nyedacnsvaxuwecedu david nye neurology consult...  \n",
       "18291  isolated ground recepticles usually unusual co...  \n",
       "18292  installed cpu clone motherboard tried mounting...  \n",
       "18293  wouldnt require hypersphere space points speci...  \n",
       "18294  tip gary crum crumfcomccutahedu got phone pont...  \n",
       "\n",
       "[18295 rows x 3 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_news20_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "54ecc9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_list = []\n",
    "for i in range(len(tree_news20_df)):\n",
    "    tree_list+=tree_news20_df['tree_token'][i].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "a080f34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1753273"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f138f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemma_tree = []\n",
    "for word in tree_list:\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    lemma_tree.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "8e10c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_word_count = Counter(lemma_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "ff302ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = []\n",
    "for key, value in tree_word_count.items():\n",
    "    if tree_word_count[key] > 184:\n",
    "        freq.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "11e81c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_news20_df['tokens'] = tree_news20_df['tree_token'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "9dfb18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_row_extract(tokens):\n",
    "    token_res = []\n",
    "    for token in tokens:\n",
    "        if token in freq:\n",
    "            token_res.append(token)\n",
    "    return token_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f0195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "8cf509e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df = tree_df[tree_df['input_token']!= '']\n",
    "tree_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "id": "2f8f793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def token_ext(tokens):\n",
    "    words= []\n",
    "    for token in tokens:\n",
    "        if token in final_token:\n",
    "            words.append(token)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "id": "d1f8c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['temp_token'] = tree_df['freq_token'].apply(lambda x: ' '.join(token_ext(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "id": "d3123927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198800"
      ]
     },
     "execution_count": 1108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join(tree_df['temp_token']).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876956e",
   "metadata": {},
   "source": [
    "# 첫 번째 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "id": "bb16af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_token = pd.DataFrame(final_token , columns=['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "id": "7b18cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df.to_csv('last_tree.csv',index=False)\n",
    "use_token.to_csv('use_token.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "id": "49f797ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time waste: 879.4023396968842 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "encoder1_res = []\n",
    "\n",
    "try:\n",
    "    for i in range(len(tree_df)):\n",
    "        encoder1_res.append(preprocess_and_layer_embedding(tree_df['temp_token'][i],0))\n",
    "except IndexError:\n",
    "    print(20000-i)\n",
    "    \n",
    "        \n",
    "print(f'time waste: {time.time()-start} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "id": "b2d8792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_tree = pd.DataFrame(np.round(np.vstack(encoder1_res),2))\n",
    "\n",
    "layer1_tree.to_csv('layer1_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "id": "52d87d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_tree.to_csv('layer1_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175e0a8",
   "metadata": {},
   "source": [
    "# 두 번째 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "id": "c874e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time waste: 795.6176817417145 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "encoder2_res = []\n",
    "\n",
    "try:\n",
    "    for i in range(len(tree_df)):\n",
    "        encoder2_res.append(preprocess_and_layer_embedding(tree_df['temp_token'][i],1))\n",
    "except IndexError:\n",
    "    print(20000-i)\n",
    "    \n",
    "        \n",
    "print(f'time waste: {time.time()-start} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "id": "1f9afdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_tree = pd.DataFrame(np.round(np.vstack(encoder2_res),2))\n",
    "\n",
    "layer2_tree.to_csv('layer2_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914df6c",
   "metadata": {},
   "source": [
    "# 세 번째 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "id": "7212b7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time waste: 931.5227339267731 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "encoder3_res = []\n",
    "\n",
    "try:\n",
    "    for i in range(len(tree_df)):\n",
    "        encoder3_res.append(preprocess_and_layer_embedding(tree_df['temp_token'][i],2))\n",
    "except IndexError:\n",
    "    print(20000-i)\n",
    "    \n",
    "        \n",
    "print(f'time waste: {time.time()-start} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "id": "84673300",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_tree = pd.DataFrame(np.round(np.vstack(encoder3_res),2))\n",
    "\n",
    "layer3_tree.to_csv('layer3_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a465d4",
   "metadata": {},
   "source": [
    "# 네 번째 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "id": "8ade086a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time waste: 987.5909519195557 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "encoder4_res = []\n",
    "\n",
    "try:\n",
    "    for i in range(len(tree_df)):\n",
    "        encoder4_res.append(preprocess_and_layer_embedding(tree_df['temp_token'][i],3))\n",
    "except IndexError:\n",
    "    print(20000-i)\n",
    "    \n",
    "        \n",
    "print(f'time waste: {time.time()-start} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "id": "370e1007",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4_tree = pd.DataFrame(np.round(np.vstack(encoder4_res),2))\n",
    "\n",
    "layer4_tree.to_csv('layer4_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a77b69",
   "metadata": {},
   "source": [
    "# 다섯 번째 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "id": "d78885fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time waste: 1099.4469830989838 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "encoder5_res = []\n",
    "\n",
    "try:\n",
    "    for i in range(len(tree_df)):\n",
    "        encoder5_res.append(preprocess_and_layer_embedding(tree_df['temp_token'][i],4))\n",
    "except IndexError:\n",
    "    print(20000-i)\n",
    "    \n",
    "        \n",
    "print(f'time waste: {time.time()-start} ')\n",
    "# 소요시간: 1151.9720940589905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "id": "dd2a708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer5_tree = pd.DataFrame(np.round(np.vstack(encoder5_res),2))\n",
    "\n",
    "layer5_tree.to_csv('layer5_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a94892a",
   "metadata": {},
   "source": [
    "# 여섯 번째 인코더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "id": "a1e6f619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time waste: 992.739339351654 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "encoder6_res = []\n",
    "\n",
    "try:\n",
    "    for i in range(len(tree_df)):\n",
    "        encoder6_res.append(preprocess_and_layer_embedding(tree_df['temp_token'][i],5))\n",
    "except IndexError:\n",
    "    print(20000-i)\n",
    "    \n",
    "        \n",
    "print(f'time waste: {time.time()-start} ')\n",
    "# 소요시간: 1151.9720940589905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "id": "3c847de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer6_tree = pd.DataFrame(np.round(np.vstack(encoder6_res),2))\n",
    "\n",
    "layer6_tree.to_csv('layer6_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97415b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece7a816",
   "metadata": {},
   "source": [
    "# 코사인 거리 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3703be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = ['topic 2','topic 3','topic 5','topic 7','topic 10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eff3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_per_layer_dist = pd.DataFrame(np.random.rand(6,5), columns = topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3409e8ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.253368</td>\n",
       "      <td>0.458148</td>\n",
       "      <td>0.892645</td>\n",
       "      <td>0.541871</td>\n",
       "      <td>0.392063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.995213</td>\n",
       "      <td>0.799812</td>\n",
       "      <td>0.391517</td>\n",
       "      <td>0.463841</td>\n",
       "      <td>0.407485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.174025</td>\n",
       "      <td>0.895917</td>\n",
       "      <td>0.791941</td>\n",
       "      <td>0.840739</td>\n",
       "      <td>0.681919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250685</td>\n",
       "      <td>0.024683</td>\n",
       "      <td>0.692406</td>\n",
       "      <td>0.775428</td>\n",
       "      <td>0.739078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.367198</td>\n",
       "      <td>0.656454</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>0.725390</td>\n",
       "      <td>0.882068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.628778</td>\n",
       "      <td>0.305990</td>\n",
       "      <td>0.457222</td>\n",
       "      <td>0.319986</td>\n",
       "      <td>0.723554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic 2   topic 3   topic 5   topic 7  topic 10\n",
       "0  0.253368  0.458148  0.892645  0.541871  0.392063\n",
       "1  0.995213  0.799812  0.391517  0.463841  0.407485\n",
       "2  0.174025  0.895917  0.791941  0.840739  0.681919\n",
       "3  0.250685  0.024683  0.692406  0.775428  0.739078\n",
       "4  0.367198  0.656454  0.019366  0.725390  0.882068\n",
       "5  0.628778  0.305990  0.457222  0.319986  0.723554"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_per_layer_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38cd7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_tree =pd.read_csv('layer1_tree.csv')\n",
    "layer2_tree =pd.read_csv('layer2_tree.csv')\n",
    "layer3_tree =pd.read_csv('layer3_tree.csv')\n",
    "layer4_tree =pd.read_csv('layer4_tree.csv')\n",
    "layer5_tree =pd.read_csv('layer5_tree.csv')\n",
    "layer6_tree =pd.read_csv('layer6_tree.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c293e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_token = pd.read_csv('use_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7311d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd046bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91bf8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_dict = {word : value for word,value in zip(list(use_token['token']), np.array(layer1_tree.values))}\n",
    "layer2_dict = {word : value for word,value in zip(list(use_token['token']), np.array(layer2_tree.values))}\n",
    "layer3_dict = {word : value for word,value in zip(list(use_token['token']), np.array(layer3_tree.values))}\n",
    "layer4_dict = {word : value for word,value in zip(list(use_token['token']), np.array(layer4_tree.values))}\n",
    "layer5_dict = {word : value for word,value in zip(list(use_token['token']), np.array(layer5_tree.values))}\n",
    "layer6_dict = {word : value for word,value in zip(list(use_token['token']), np.array(layer6_tree.values))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff3bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = [layer1_dict,layer2_dict,layer3_dict,layer4_dict,layer5_dict,layer6_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b70d82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_cosine_dist(use_df, num_topic,idx):\n",
    "    df = copy.copy(use_df)\n",
    "    gmm = GaussianMixture(n_components=num_topic, random_state=43)\n",
    "    gmm.fit(np.array(df)) # GMM 클러스터링   \n",
    "    df['Cluster'] = gmm.predict(np.array(df))\n",
    "\n",
    "    labels = df['Cluster']\n",
    "    all_tokens = use_token['token']\n",
    "    \n",
    "    cluster_words = [[] for _ in range(df.Cluster.nunique())]\n",
    "    for token, label in zip(all_tokens, labels):\n",
    "         cluster_words[label].append(token)\n",
    "\n",
    "    representative_words = []\n",
    "    for cluster in cluster_words:\n",
    "        word_counts = Counter(cluster)\n",
    "        most_5_words_only = [word for word, count in word_counts.most_common(5)]  #top 10 word\n",
    "        representative_words.append(most_5_words_only)\n",
    "\n",
    "    per_cluster = [representative_words[i] for i in range(df.Cluster.nunique())] \n",
    "#     all_cluster = [cluster_words[i] for i in range(df.Cluster.nunique())] \n",
    "#     dis_res = [[] for _ in range(df.Cluster.nunique())]\n",
    "\n",
    "\n",
    "    dis_res = []  # \n",
    "#     cluster = gmm_words(use_df, num_topic,idx)\n",
    "    \n",
    "    for cluster in per_cluster:\n",
    "        mean_res = []\n",
    "        distance = []\n",
    "        for w1, w2 in combinations(cluster,2):\n",
    "            distance.append(cosine_distance_dstbert(w1,w2,idx))  # 여기를 고쳐야 함 \n",
    "        mean_res = np.mean(distance)\n",
    "        dis_res.append(mean_res)\n",
    "        \n",
    "    return np.round(np.mean(dis_res),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac22450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_dict는 각 layer마다 단어에 대응하는 임베딩 벡터를 딕셔너리 형태로 저장한 뒤 각 딕셔너리를 하나의 리스트로 넣은 객체를 의미함\n",
    "\n",
    "def cosine_distance_dstbert(x,y, idx):\n",
    "     \n",
    "    vector1 = layer_dict[idx][x]\n",
    "    vector2 = layer_dict[idx][y]\n",
    "    \n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    \n",
    "    cosine_similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    cosine_distance = np.round(1 - cosine_similarity, 3)\n",
    "    \n",
    "    return cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edfdcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab2886eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m num_topics:    \n\u001b[1;32m----> 8\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgmm_cosine_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      9\u001b[0m distilbert_per_layer_dist\u001b[38;5;241m.\u001b[39mloc[layer] \u001b[38;5;241m=\u001b[39m res\n",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m, in \u001b[0;36mgmm_cosine_dist\u001b[1;34m(use_df, num_topic, idx)\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(use_df)\n\u001b[0;32m      3\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39mnum_topic, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m43\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# GMM 클러스터링   \u001b[39;00m\n\u001b[0;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(df))\n\u001b[0;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_base.py:186\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_base.py:253\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    250\u001b[0m prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[0;32m    252\u001b[0m log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(X)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n\u001b[0;32m    256\u001b[0m change \u001b[38;5;241m=\u001b[39m lower_bound \u001b[38;5;241m-\u001b[39m prev_lower_bound\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:753\u001b[0m, in \u001b[0;36mGaussianMixture._m_step\u001b[1;34m(self, X, log_resp)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, log_resp):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;124;03m\"\"\"M step.\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_ \u001b[38;5;241m=\u001b[39m \u001b[43m_estimate_gaussian_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreg_covar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecisions_cholesky_ \u001b[38;5;241m=\u001b[39m _compute_precision_cholesky(\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[0;32m    759\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:292\u001b[0m, in \u001b[0;36m_estimate_gaussian_parameters\u001b[1;34m(X, resp, reg_covar, covariance_type)\u001b[0m\n\u001b[0;32m    290\u001b[0m nk \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(resp\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[0;32m    291\u001b[0m means \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(resp\u001b[38;5;241m.\u001b[39mT, X) \u001b[38;5;241m/\u001b[39m nk[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m--> 292\u001b[0m covariances \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfull\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_full\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtied\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_tied\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_diag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspherical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_estimate_gaussian_covariances_spherical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcovariance_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_covar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nk, means, covariances\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:179\u001b[0m, in \u001b[0;36m_estimate_gaussian_covariances_full\u001b[1;34m(resp, X, nk, means, reg_covar)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_components):\n\u001b[0;32m    178\u001b[0m     diff \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m-\u001b[39m means[k]\n\u001b[1;32m--> 179\u001b[0m     covariances[k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nk[k]\n\u001b[0;32m    180\u001b[0m     covariances[k]\u001b[38;5;241m.\u001b[39mflat[:: n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reg_covar\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m covariances\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# num_topics = [2,5,10,15,20,25,30,35,40,45,50,55]\n",
    "num_topics = 2\n",
    "seq_df = [layer1_tree, layer2_tree, layer3_tree, layer4_tree, layer5_tree, layer6_tree]\n",
    "\n",
    "for layer,df in enumerate(seq_df):\n",
    "    res = []\n",
    "    for topic in num_topics:    \n",
    "        res.append(gmm_cosine_dist(df, topic, layer))\n",
    "    distilbert_per_layer_dist.loc[layer] = res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d111c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_cos_dist = pd.DataFrame(np.random.rand(6,1), columns = ['topic 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "835a80c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.467557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.268516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.114624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.366197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.246798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.590459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic 2\n",
       "0  0.467557\n",
       "1  0.268516\n",
       "2  0.114624\n",
       "3  0.366197\n",
       "4  0.246798\n",
       "5  0.590459"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst_cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6e3cced",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer,df \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq_df):\n\u001b[0;32m      2\u001b[0m     res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m     res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgmm_cosine_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m     dst_cos_dist\u001b[38;5;241m.\u001b[39mloc[layer] \u001b[38;5;241m=\u001b[39m res\n",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m, in \u001b[0;36mgmm_cosine_dist\u001b[1;34m(use_df, num_topic, idx)\u001b[0m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(use_df)\n\u001b[0;32m      3\u001b[0m gmm \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39mnum_topic, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m43\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mgmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# GMM 클러스터링   \u001b[39;00m\n\u001b[0;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gmm\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(df))\n\u001b[0;32m      7\u001b[0m labels \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCluster\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_base.py:186\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_base.py:252\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    250\u001b[0m     prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[1;32m--> 252\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_m_step(X, log_resp)\n\u001b[0;32m    254\u001b[0m     lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_base.py:309\u001b[0m, in \u001b[0;36mBaseMixture._e_step\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_e_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;124;03m\"\"\"E step.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob_resp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(log_prob_norm), log_resp\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_base.py:529\u001b[0m, in \u001b[0;36mBaseMixture._estimate_log_prob_resp\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_log_prob_resp\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate log probabilities and responsibilities for each sample.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    Compute the log probabilities, weighted log probabilities per\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m        logarithm of the responsibilities\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m     weighted_log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_weighted_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m     log_prob_norm \u001b[38;5;241m=\u001b[39m logsumexp(weighted_log_prob, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(under\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;66;03m# ignore underflow\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_base.py:482\u001b[0m, in \u001b[0;36mBaseMixture._estimate_weighted_log_prob\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_weighted_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate the weighted log-probabilities, log P(X | Z) + log weights.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m    weighted_log_prob : array, shape (n_samples, n_component)\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_estimate_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_estimate_log_weights()\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:762\u001b[0m, in \u001b[0;36mGaussianMixture._estimate_log_prob\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_log_gaussian_prob\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeans_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecisions_cholesky_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcovariance_type\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\text\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:428\u001b[0m, in \u001b[0;36m_estimate_log_gaussian_prob\u001b[1;34m(X, means, precisions_chol, covariance_type)\u001b[0m\n\u001b[0;32m    426\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((n_samples, n_components))\n\u001b[0;32m    427\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, (mu, prec_chol) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(means, precisions_chol)):\n\u001b[1;32m--> 428\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec_chol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(mu, prec_chol)\n\u001b[0;32m    429\u001b[0m         log_prob[:, k] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msquare(y), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m covariance_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtied\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for layer,df in enumerate(seq_df):\n",
    "    res = []\n",
    "    res.append(gmm_cosine_dist(df, 2, layer))\n",
    "    dst_cos_dist.loc[layer] = res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69141cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11bdb663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.949800</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>0.914100</td>\n",
       "      <td>0.889700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.745900</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.174025</td>\n",
       "      <td>0.895917</td>\n",
       "      <td>0.791941</td>\n",
       "      <td>0.840739</td>\n",
       "      <td>0.681919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.250685</td>\n",
       "      <td>0.024683</td>\n",
       "      <td>0.692406</td>\n",
       "      <td>0.775428</td>\n",
       "      <td>0.739078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.367198</td>\n",
       "      <td>0.656454</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>0.725390</td>\n",
       "      <td>0.882068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.628778</td>\n",
       "      <td>0.305990</td>\n",
       "      <td>0.457222</td>\n",
       "      <td>0.319986</td>\n",
       "      <td>0.723554</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic 2   topic 3   topic 5   topic 7  topic 10\n",
       "0  0.949800  0.928500  0.914100  0.889700       NaN\n",
       "1  0.745900  0.745900  0.752000       NaN  0.611500\n",
       "2  0.174025  0.895917  0.791941  0.840739  0.681919\n",
       "3  0.250685  0.024683  0.692406  0.775428  0.739078\n",
       "4  0.367198  0.656454  0.019366  0.725390  0.882068\n",
       "5  0.628778  0.305990  0.457222  0.319986  0.723554"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_per_layer_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0e2d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a6638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "212de2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_tree = pd.DataFrame(np.round(np.vstack(encoder1_res),2))\n",
    "\n",
    "# layer1_tree.to_csv('new_task/layer1_tree.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "c2ebb864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.49</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>1.05</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>1.08</td>\n",
       "      <td>-0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-1.31</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198892</th>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>1.03</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198893</th>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.19</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198894</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>-1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198895</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198896</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.58</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>1.55</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.69</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198897 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2     3     4     5     6     7     8     9    ...   758  \\\n",
       "0      -0.33 -0.79 -0.14 -0.01 -0.41  0.24  0.25  0.30 -0.10 -0.04  ...  0.25   \n",
       "1       0.08  0.66 -0.56 -0.77 -0.21 -0.48  0.65 -0.31  0.90 -0.50  ...  0.57   \n",
       "2      -1.49  0.73  0.29 -0.09  1.05 -0.26  0.26 -0.48 -0.71 -0.07  ...  0.53   \n",
       "3       0.48 -0.91  0.05 -0.01  0.26 -0.27 -2.22 -0.61 -0.32 -0.19  ...  2.29   \n",
       "4      -0.41  0.64  0.25  0.46 -0.79  0.99  0.47 -0.50  0.34 -0.53  ...  0.54   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "198892 -0.17  1.23  0.01 -0.16 -0.35  1.24 -0.40 -0.63  1.03 -0.40  ... -0.10   \n",
       "198893 -0.51  0.14 -0.75 -0.17 -0.29  0.56  1.04  1.45 -0.06 -0.48  ...  0.88   \n",
       "198894  1.19  0.29 -0.39  0.43  1.45 -0.58  0.15 -0.35 -1.28 -0.11  ... -0.49   \n",
       "198895  0.47  0.81  0.05 -0.78 -0.38  0.80  0.91  1.24  0.13  0.82  ... -0.00   \n",
       "198896 -0.00 -0.59 -0.64  0.22  0.68  0.14  0.58 -0.35  1.55 -0.16  ...  0.38   \n",
       "\n",
       "         759   760   761   762   763   764   765   766   767  \n",
       "0      -0.18 -0.33 -0.23  0.13  0.09  0.27 -0.17 -0.11  0.64  \n",
       "1       0.21  0.36 -0.81 -0.28  0.77 -0.10  0.03  0.64  0.37  \n",
       "2       0.96 -0.04 -0.21  0.89  0.12 -0.11 -0.01  1.08 -0.38  \n",
       "3      -0.11 -1.31 -0.70  1.08  1.08  1.03 -0.15 -0.61  0.14  \n",
       "4       0.86 -0.33  0.48 -0.26 -0.29  0.43  0.40 -0.84 -0.28  \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "198892  0.59  0.15  0.53  0.04  0.66  0.34  0.28 -1.00 -0.92  \n",
       "198893 -0.67  0.54  0.11 -1.19 -0.28 -1.08  1.32  0.46 -0.40  \n",
       "198894 -0.77  0.59 -0.10 -0.45 -0.06  0.82  0.14 -0.76 -1.09  \n",
       "198895 -0.07 -0.11 -1.04 -0.09  0.41 -0.01 -0.91  0.09 -0.51  \n",
       "198896  0.49  0.15  0.29 -0.69 -0.96  0.86 -0.09  0.22  0.76  \n",
       "\n",
       "[198897 rows x 768 columns]"
      ]
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "09319f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>documentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198842</th>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198843</th>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198844</th>\n",
       "      <td>fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198845</th>\n",
       "      <td>current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198846</th>\n",
       "      <td>seek</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198847 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                token\n",
       "0                 get\n",
       "1       documentation\n",
       "2          interested\n",
       "3             welcome\n",
       "4              rainer\n",
       "...               ...\n",
       "198842      community\n",
       "198843          thing\n",
       "198844           fire\n",
       "198845        current\n",
       "198846           seek\n",
       "\n",
       "[198847 rows x 1 columns]"
      ]
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4e5bd332",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3627</td>\n",
       "      <td>-0.0396</td>\n",
       "      <td>-0.3849</td>\n",
       "      <td>-0.0843</td>\n",
       "      <td>-0.2874</td>\n",
       "      <td>-0.9049</td>\n",
       "      <td>-0.3652</td>\n",
       "      <td>0.5052</td>\n",
       "      <td>-0.5396</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5483</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>-0.1985</td>\n",
       "      <td>-0.5911</td>\n",
       "      <td>-0.2643</td>\n",
       "      <td>-0.1638</td>\n",
       "      <td>-0.3983</td>\n",
       "      <td>0.0735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5009</td>\n",
       "      <td>0.5196</td>\n",
       "      <td>0.1585</td>\n",
       "      <td>-0.0458</td>\n",
       "      <td>-0.0829</td>\n",
       "      <td>1.0928</td>\n",
       "      <td>0.3295</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>-1.3371</td>\n",
       "      <td>-1.1122</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0556</td>\n",
       "      <td>-0.1730</td>\n",
       "      <td>-1.4784</td>\n",
       "      <td>0.6247</td>\n",
       "      <td>-0.1982</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.3371</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>1.0178</td>\n",
       "      <td>0.4726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4867</td>\n",
       "      <td>-0.8400</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.2469</td>\n",
       "      <td>-0.5134</td>\n",
       "      <td>-1.3055</td>\n",
       "      <td>0.6620</td>\n",
       "      <td>1.0557</td>\n",
       "      <td>-1.1075</td>\n",
       "      <td>-0.2555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8563</td>\n",
       "      <td>0.2276</td>\n",
       "      <td>-0.1619</td>\n",
       "      <td>-0.0563</td>\n",
       "      <td>0.6245</td>\n",
       "      <td>-0.7516</td>\n",
       "      <td>-0.4063</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.6061</td>\n",
       "      <td>0.3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1257</td>\n",
       "      <td>-1.5072</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>-0.2137</td>\n",
       "      <td>1.7646</td>\n",
       "      <td>0.5419</td>\n",
       "      <td>-0.5076</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>-0.5733</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0252</td>\n",
       "      <td>0.7133</td>\n",
       "      <td>-1.0004</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>-0.1183</td>\n",
       "      <td>-0.6463</td>\n",
       "      <td>0.4016</td>\n",
       "      <td>0.2659</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>-0.6957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7069</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.6088</td>\n",
       "      <td>-0.7212</td>\n",
       "      <td>1.7846</td>\n",
       "      <td>-0.1275</td>\n",
       "      <td>-0.6274</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>0.4329</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4532</td>\n",
       "      <td>-0.0416</td>\n",
       "      <td>-0.9670</td>\n",
       "      <td>-0.7363</td>\n",
       "      <td>-0.6979</td>\n",
       "      <td>0.3368</td>\n",
       "      <td>-0.3099</td>\n",
       "      <td>-1.5636</td>\n",
       "      <td>-0.2530</td>\n",
       "      <td>-0.0689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846168</th>\n",
       "      <td>-1.0452</td>\n",
       "      <td>-1.6131</td>\n",
       "      <td>0.1631</td>\n",
       "      <td>0.1744</td>\n",
       "      <td>-1.1586</td>\n",
       "      <td>0.9393</td>\n",
       "      <td>0.5432</td>\n",
       "      <td>1.0304</td>\n",
       "      <td>-0.5147</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4332</td>\n",
       "      <td>-0.5576</td>\n",
       "      <td>-0.6701</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>-0.2576</td>\n",
       "      <td>0.7359</td>\n",
       "      <td>0.7860</td>\n",
       "      <td>-0.8857</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>0.2530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846169</th>\n",
       "      <td>-0.0913</td>\n",
       "      <td>-0.4025</td>\n",
       "      <td>-1.3338</td>\n",
       "      <td>-0.6601</td>\n",
       "      <td>0.2734</td>\n",
       "      <td>-0.7833</td>\n",
       "      <td>-0.6587</td>\n",
       "      <td>-0.6108</td>\n",
       "      <td>-0.8848</td>\n",
       "      <td>-1.0054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>-0.4096</td>\n",
       "      <td>0.1239</td>\n",
       "      <td>0.6975</td>\n",
       "      <td>-0.6692</td>\n",
       "      <td>-0.1191</td>\n",
       "      <td>1.3635</td>\n",
       "      <td>-1.2826</td>\n",
       "      <td>-1.1715</td>\n",
       "      <td>-0.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846170</th>\n",
       "      <td>-1.1877</td>\n",
       "      <td>-0.7216</td>\n",
       "      <td>0.4150</td>\n",
       "      <td>0.5930</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>-0.4539</td>\n",
       "      <td>0.4455</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.8263</td>\n",
       "      <td>0.5812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5686</td>\n",
       "      <td>-0.3015</td>\n",
       "      <td>-1.3678</td>\n",
       "      <td>0.1995</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>-1.1460</td>\n",
       "      <td>-0.3390</td>\n",
       "      <td>-0.1966</td>\n",
       "      <td>-0.0235</td>\n",
       "      <td>0.1157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846171</th>\n",
       "      <td>0.9446</td>\n",
       "      <td>-0.9358</td>\n",
       "      <td>0.1138</td>\n",
       "      <td>-0.0283</td>\n",
       "      <td>-0.4871</td>\n",
       "      <td>-0.1162</td>\n",
       "      <td>1.5715</td>\n",
       "      <td>0.2388</td>\n",
       "      <td>0.3536</td>\n",
       "      <td>0.5631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4260</td>\n",
       "      <td>-0.5510</td>\n",
       "      <td>-1.1140</td>\n",
       "      <td>-0.6504</td>\n",
       "      <td>-0.7908</td>\n",
       "      <td>0.1961</td>\n",
       "      <td>0.2836</td>\n",
       "      <td>-0.0834</td>\n",
       "      <td>0.6049</td>\n",
       "      <td>0.7435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846172</th>\n",
       "      <td>0.6204</td>\n",
       "      <td>0.4898</td>\n",
       "      <td>0.7509</td>\n",
       "      <td>-0.3812</td>\n",
       "      <td>1.0825</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>0.7062</td>\n",
       "      <td>-0.7107</td>\n",
       "      <td>0.3821</td>\n",
       "      <td>-0.8891</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0849</td>\n",
       "      <td>-0.5595</td>\n",
       "      <td>-1.2804</td>\n",
       "      <td>0.5403</td>\n",
       "      <td>1.1631</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>-0.2475</td>\n",
       "      <td>-1.3634</td>\n",
       "      <td>-0.6139</td>\n",
       "      <td>1.6777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>846173 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0       1       2       3       4       5       6       7    \\\n",
       "0       0.3627 -0.0396 -0.3849 -0.0843 -0.2874 -0.9049 -0.3652  0.5052   \n",
       "1       0.5009  0.5196  0.1585 -0.0458 -0.0829  1.0928  0.3295  0.3534   \n",
       "2       0.4867 -0.8400  0.0434  0.2469 -0.5134 -1.3055  0.6620  1.0557   \n",
       "3      -0.1257 -1.5072  0.1497 -0.2137  1.7646  0.5419 -0.5076  0.7426   \n",
       "4       0.7069  0.3420  0.1325  0.6088 -0.7212  1.7846 -0.1275 -0.6274   \n",
       "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "846168 -1.0452 -1.6131  0.1631  0.1744 -1.1586  0.9393  0.5432  1.0304   \n",
       "846169 -0.0913 -0.4025 -1.3338 -0.6601  0.2734 -0.7833 -0.6587 -0.6108   \n",
       "846170 -1.1877 -0.7216  0.4150  0.5930  0.8136 -0.4539  0.4455  0.1399   \n",
       "846171  0.9446 -0.9358  0.1138 -0.0283 -0.4871 -0.1162  1.5715  0.2388   \n",
       "846172  0.6204  0.4898  0.7509 -0.3812  1.0825  0.3512  0.7062 -0.7107   \n",
       "\n",
       "           8       9    ...     758     759     760     761     762     763  \\\n",
       "0      -0.5396  0.0487  ... -0.5483  0.0440  0.3418  0.0755 -0.1985 -0.5911   \n",
       "1      -1.3371 -1.1122  ...  1.0556 -0.1730 -1.4784  0.6247 -0.1982  0.1900   \n",
       "2      -1.1075 -0.2555  ...  0.8563  0.2276 -0.1619 -0.0563  0.6245 -0.7516   \n",
       "3       0.3555 -0.5733  ...  1.0252  0.7133 -1.0004  0.6013 -0.1183 -0.6463   \n",
       "4       0.9302  0.4329  ...  0.4532 -0.0416 -0.9670 -0.7363 -0.6979  0.3368   \n",
       "...        ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "846168 -0.5147  0.9168  ...  0.4332 -0.5576 -0.6701  0.0441 -0.2576  0.7359   \n",
       "846169 -0.8848 -1.0054  ...  0.2384 -0.4096  0.1239  0.6975 -0.6692 -0.1191   \n",
       "846170  0.8263  0.5812  ... -0.5686 -0.3015 -1.3678  0.1995  0.4939 -1.1460   \n",
       "846171  0.3536  0.5631  ... -0.4260 -0.5510 -1.1140 -0.6504 -0.7908  0.1961   \n",
       "846172  0.3821 -0.8891  ... -1.0849 -0.5595 -1.2804  0.5403  1.1631  0.9051   \n",
       "\n",
       "           764     765     766     767  \n",
       "0      -0.2643 -0.1638 -0.3983  0.0735  \n",
       "1       0.3371  0.5053  1.0178  0.4726  \n",
       "2      -0.4063  0.0458  0.6061  0.3755  \n",
       "3       0.4016  0.2659  0.4624 -0.6957  \n",
       "4      -0.3099 -1.5636 -0.2530 -0.0689  \n",
       "...        ...     ...     ...     ...  \n",
       "846168  0.7860 -0.8857  0.1659  0.2530  \n",
       "846169  1.3635 -1.2826 -1.1715 -0.0137  \n",
       "846170 -0.3390 -0.1966 -0.0235  0.1157  \n",
       "846171  0.2836 -0.0834  0.6049  0.7435  \n",
       "846172 -0.2475 -1.3634 -0.6139  1.6777  \n",
       "\n",
       "[846173 rows x 768 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c6824200",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_df['last_join'] = tree_df['last_token'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "1170356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = []\n",
    "for i in range(len(tree_df)):\n",
    "    full+=tree_df['last_join'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "63be1363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tree_token</th>\n",
       "      <th>tree_ref_token</th>\n",
       "      <th>last_token</th>\n",
       "      <th>last_join</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>[sure, bashers, pens, fans, pretty, confused, ...</td>\n",
       "      <td>sure bashers pens fans pretty confused lack ki...</td>\n",
       "      <td>sure pretty lack kind recent massacre actually...</td>\n",
       "      <td>[sure, pretty, lack, kind, recent, massacre, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>[market, highperformance, video, card, support...</td>\n",
       "      <td>market highperformance video card supports ves...</td>\n",
       "      <td>market video card local bus ram anyone pro loc...</td>\n",
       "      <td>[market, video, card, local, bus, ram, anyone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>[finally, said, dream, mediterranean, new, are...</td>\n",
       "      <td>finally said dream mediterranean new area grea...</td>\n",
       "      <td>finally said new area greater like usa april s...</td>\n",
       "      <td>[finally, said, new, area, greater, like, usa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>[think, scsi, card, dma, transfers, disks, scs...</td>\n",
       "      <td>think scsi card dma transfers disks scsi card ...</td>\n",
       "      <td>think scsi card scsi card data scsi important ...</td>\n",
       "      <td>[think, scsi, card, scsi, card, data, scsi, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>[old, jasmine, drive, use, new, system, unders...</td>\n",
       "      <td>old jasmine drive use new system understanding...</td>\n",
       "      <td>old drive use new system understanding driver ...</td>\n",
       "      <td>[old, drive, use, new, system, understanding, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18170</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>13</td>\n",
       "      <td>[nyedacnsvaxuwecedu, david, nye, neurology, co...</td>\n",
       "      <td>nyedacnsvaxuwecedu david nye neurology consult...</td>\n",
       "      <td>david also better make appear normal also reco...</td>\n",
       "      <td>[david, also, better, make, appear, normal, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18171</th>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>12</td>\n",
       "      <td>[isolated, ground, recepticles, usually, unusu...</td>\n",
       "      <td>isolated ground recepticles usually unusual co...</td>\n",
       "      <td>ground usually color often used low noise low ...</td>\n",
       "      <td>[ground, usually, color, often, used, low, noi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18172</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "      <td>[installed, cpu, clone, motherboard, tried, mo...</td>\n",
       "      <td>installed cpu clone motherboard tried mounting...</td>\n",
       "      <td>installed cpu motherboard tried cpu chip hour ...</td>\n",
       "      <td>[installed, cpu, motherboard, tried, cpu, chip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18173</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>1</td>\n",
       "      <td>[wouldnt, require, hypersphere, space, points,...</td>\n",
       "      <td>wouldnt require hypersphere space points speci...</td>\n",
       "      <td>wouldnt require space far see unless prove poi...</td>\n",
       "      <td>[wouldnt, require, space, far, see, unless, pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18174</th>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>7</td>\n",
       "      <td>[tip, gary, crum, crumfcomccutahedu, got, phon...</td>\n",
       "      <td>tip gary crum crumfcomccutahedu got phone pont...</td>\n",
       "      <td>got phone service whatever hold btw talking de...</td>\n",
       "      <td>[got, phone, service, whatever, hold, btw, tal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18175 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target  \\\n",
       "0      \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1      My brother is in the market for a high-perform...       3   \n",
       "2      \\n\\n\\n\\n\\tFinally you said what you dream abou...      17   \n",
       "3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3   \n",
       "4      1)    I have an old Jasmine drive which I cann...       4   \n",
       "...                                                  ...     ...   \n",
       "18170  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...      13   \n",
       "18171  \\nNot in isolated ground recepticles (usually ...      12   \n",
       "18172  I just installed a DX2-66 CPU in a clone mothe...       3   \n",
       "18173  \\nWouldn't this require a hyper-sphere.  In 3-...       1   \n",
       "18174  After a tip from Gary Crum (crum@fcom.cc.utah....       7   \n",
       "\n",
       "                                              tree_token  \\\n",
       "0      [sure, bashers, pens, fans, pretty, confused, ...   \n",
       "1      [market, highperformance, video, card, support...   \n",
       "2      [finally, said, dream, mediterranean, new, are...   \n",
       "3      [think, scsi, card, dma, transfers, disks, scs...   \n",
       "4      [old, jasmine, drive, use, new, system, unders...   \n",
       "...                                                  ...   \n",
       "18170  [nyedacnsvaxuwecedu, david, nye, neurology, co...   \n",
       "18171  [isolated, ground, recepticles, usually, unusu...   \n",
       "18172  [installed, cpu, clone, motherboard, tried, mo...   \n",
       "18173  [wouldnt, require, hypersphere, space, points,...   \n",
       "18174  [tip, gary, crum, crumfcomccutahedu, got, phon...   \n",
       "\n",
       "                                          tree_ref_token  \\\n",
       "0      sure bashers pens fans pretty confused lack ki...   \n",
       "1      market highperformance video card supports ves...   \n",
       "2      finally said dream mediterranean new area grea...   \n",
       "3      think scsi card dma transfers disks scsi card ...   \n",
       "4      old jasmine drive use new system understanding...   \n",
       "...                                                  ...   \n",
       "18170  nyedacnsvaxuwecedu david nye neurology consult...   \n",
       "18171  isolated ground recepticles usually unusual co...   \n",
       "18172  installed cpu clone motherboard tried mounting...   \n",
       "18173  wouldnt require hypersphere space points speci...   \n",
       "18174  tip gary crum crumfcomccutahedu got phone pont...   \n",
       "\n",
       "                                              last_token  \\\n",
       "0      sure pretty lack kind recent massacre actually...   \n",
       "1      market video card local bus ram anyone pro loc...   \n",
       "2      finally said new area greater like usa april s...   \n",
       "3      think scsi card scsi card data scsi important ...   \n",
       "4      old drive use new system understanding driver ...   \n",
       "...                                                  ...   \n",
       "18170  david also better make appear normal also reco...   \n",
       "18171  ground usually color often used low noise low ...   \n",
       "18172  installed cpu motherboard tried cpu chip hour ...   \n",
       "18173  wouldnt require space far see unless prove poi...   \n",
       "18174  got phone service whatever hold btw talking de...   \n",
       "\n",
       "                                               last_join  \n",
       "0      [sure, pretty, lack, kind, recent, massacre, a...  \n",
       "1      [market, video, card, local, bus, ram, anyone,...  \n",
       "2      [finally, said, new, area, greater, like, usa,...  \n",
       "3      [think, scsi, card, scsi, card, data, scsi, im...  \n",
       "4      [old, drive, use, new, system, understanding, ...  \n",
       "...                                                  ...  \n",
       "18170  [david, also, better, make, appear, normal, al...  \n",
       "18171  [ground, usually, color, often, used, low, noi...  \n",
       "18172  [installed, cpu, motherboard, tried, cpu, chip...  \n",
       "18173  [wouldnt, require, space, far, see, unless, pr...  \n",
       "18174  [got, phone, service, whatever, hold, btw, tal...  \n",
       "\n",
       "[18175 rows x 6 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5995dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "7383e6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text              Inguiry by address:er1@eridan.chuvashia.su\\n\n",
       "target                                                       6\n",
       "tree_token             [inguiry, addresser, eridanchuvashiasu]\n",
       "tree_ref_token             inguiry addresser eridanchuvashiasu\n",
       "last_token                                                  []\n",
       "Name: 198, dtype: object"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_df.loc[198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5aa842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "cae8133d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>sure bash ##ers pens fans pretty confused lack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>market high ##per ##form ##ance video card sup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>finally said dream mediterranean new area grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>think ##si card ##ma transfers disks ##si card...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>old jasmine drive cannot use new system unders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18290</th>\n",
       "      <td>DN&gt; From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...</td>\n",
       "      <td>13</td>\n",
       "      <td>##n ##eda ##c ##ns ##va ##x ##uw ##ece ##du da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18291</th>\n",
       "      <td>\\nNot in isolated ground recepticles (usually ...</td>\n",
       "      <td>12</td>\n",
       "      <td>isolated ground rec ##ept ##icles usually unus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18292</th>\n",
       "      <td>I just installed a DX2-66 CPU in a clone mothe...</td>\n",
       "      <td>3</td>\n",
       "      <td>installed ##x cpu clone mother ##board tried m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18293</th>\n",
       "      <td>\\nWouldn't this require a hyper-sphere.  In 3-...</td>\n",
       "      <td>1</td>\n",
       "      <td>##t require hyper ##sphere space points specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18294</th>\n",
       "      <td>After a tip from Gary Crum (crum@fcom.cc.utah....</td>\n",
       "      <td>7</td>\n",
       "      <td>tip gary ##um ##um ##fc ##om ##cc ##uta ##hed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18295 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target  \\\n",
       "0      \\n\\nI am sure some bashers of Pens fans are pr...      10   \n",
       "1      My brother is in the market for a high-perform...       3   \n",
       "2      \\n\\n\\n\\n\\tFinally you said what you dream abou...      17   \n",
       "3      \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3   \n",
       "4      1)    I have an old Jasmine drive which I cann...       4   \n",
       "...                                                  ...     ...   \n",
       "18290  DN> From: nyeda@cnsvax.uwec.edu (David Nye)\\nD...      13   \n",
       "18291  \\nNot in isolated ground recepticles (usually ...      12   \n",
       "18292  I just installed a DX2-66 CPU in a clone mothe...       3   \n",
       "18293  \\nWouldn't this require a hyper-sphere.  In 3-...       1   \n",
       "18294  After a tip from Gary Crum (crum@fcom.cc.utah....       7   \n",
       "\n",
       "                                                   token  \n",
       "0      sure bash ##ers pens fans pretty confused lack...  \n",
       "1      market high ##per ##form ##ance video card sup...  \n",
       "2      finally said dream mediterranean new area grea...  \n",
       "3      think ##si card ##ma transfers disks ##si card...  \n",
       "4      old jasmine drive cannot use new system unders...  \n",
       "...                                                  ...  \n",
       "18290  ##n ##eda ##c ##ns ##va ##x ##uw ##ece ##du da...  \n",
       "18291  isolated ground rec ##ept ##icles usually unus...  \n",
       "18292  installed ##x cpu clone mother ##board tried m...  \n",
       "18293  ##t require hyper ##sphere space points specif...  \n",
       "18294  tip gary ##um ##um ##fc ##om ##cc ##uta ##hed ...  \n",
       "\n",
       "[18295 rows x 3 columns]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news20_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b114490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7dd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1a712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f5e1395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_top_words(use_df, num_topic):\n",
    "    df = copy.copy(use_df)\n",
    "    gmm = GaussianMixture(n_components=num_topic, random_state=43)\n",
    "    gmm.fit(np.array(df)) # GMM 클러스터링   \n",
    "    df['Cluster'] = gmm.predict(np.array(df))\n",
    "\n",
    "    labels = df['Cluster']\n",
    "    all_tokens = use_token['token']\n",
    "    \n",
    "    cluster_words = [[] for _ in range(df.Cluster.nunique())]\n",
    "    for token, label in zip(all_tokens, labels):\n",
    "         cluster_words[label].append(token)\n",
    "\n",
    "    representative_words = []\n",
    "    for cluster in cluster_words:\n",
    "        word_counts = Counter(cluster)\n",
    "        most_10_words_only = [word for word, count in word_counts.most_common(10)]  #top 10 word\n",
    "        representative_words.append(most_10_words_only)\n",
    "\n",
    "    per_cluster = [representative_words[i] for i in range(df.Cluster.nunique())] \n",
    "    \n",
    "    \n",
    "    return per_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5735c892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['please',\n",
       "  'problem',\n",
       "  'system',\n",
       "  'email',\n",
       "  'information',\n",
       "  'question',\n",
       "  'program',\n",
       "  'available',\n",
       "  'drive',\n",
       "  'able'],\n",
       " ['get',\n",
       "  'time',\n",
       "  'good',\n",
       "  'anyone',\n",
       "  'use',\n",
       "  'way',\n",
       "  'new',\n",
       "  'many',\n",
       "  'something',\n",
       "  'much']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_top_words(layer1_tree,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf4046fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['time',\n",
       "  'use',\n",
       "  'problem',\n",
       "  'system',\n",
       "  'work',\n",
       "  'email',\n",
       "  'point',\n",
       "  'information',\n",
       "  'bit',\n",
       "  'number'],\n",
       " ['get',\n",
       "  'good',\n",
       "  'way',\n",
       "  'new',\n",
       "  'anyone',\n",
       "  'many',\n",
       "  'something',\n",
       "  'please',\n",
       "  'much',\n",
       "  'last']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_top_words(layer2_tree,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99081859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['get',\n",
       "  'time',\n",
       "  'good',\n",
       "  'anyone',\n",
       "  'use',\n",
       "  'way',\n",
       "  'something',\n",
       "  'please',\n",
       "  'problem',\n",
       "  'much'],\n",
       " ['new',\n",
       "  'many',\n",
       "  'system',\n",
       "  'last',\n",
       "  'point',\n",
       "  'year',\n",
       "  'question',\n",
       "  'information',\n",
       "  'bit',\n",
       "  'number']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_top_words(layer3_tree,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae5174a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['get',\n",
       "  'time',\n",
       "  'good',\n",
       "  'anyone',\n",
       "  'use',\n",
       "  'way',\n",
       "  'many',\n",
       "  'something',\n",
       "  'please',\n",
       "  'problem'],\n",
       " ['new',\n",
       "  'system',\n",
       "  'last',\n",
       "  'point',\n",
       "  'year',\n",
       "  'number',\n",
       "  'power',\n",
       "  'program',\n",
       "  'first',\n",
       "  'post']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_top_words(layer4_tree,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1745bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['get',\n",
       "  'time',\n",
       "  'good',\n",
       "  'anyone',\n",
       "  'use',\n",
       "  'way',\n",
       "  'many',\n",
       "  'something',\n",
       "  'please',\n",
       "  'problem'],\n",
       " ['new',\n",
       "  'system',\n",
       "  'last',\n",
       "  'year',\n",
       "  'power',\n",
       "  'program',\n",
       "  'first',\n",
       "  'post',\n",
       "  'news',\n",
       "  'old']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_top_words(layer5_tree,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1d96c470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['get',\n",
       "  'anyone',\n",
       "  'good',\n",
       "  'use',\n",
       "  'time',\n",
       "  'something',\n",
       "  'many',\n",
       "  'way',\n",
       "  'problem',\n",
       "  'much'],\n",
       " ['new', 'time', 'year', 'last', 'first', 'car', 'old', 'game', 'high', 'way']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_top_words(layer6_tree,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8666d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>documentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interested</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198795</th>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198796</th>\n",
       "      <td>thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198797</th>\n",
       "      <td>fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198798</th>\n",
       "      <td>current</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198799</th>\n",
       "      <td>seek</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198800 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                token\n",
       "0                 get\n",
       "1       documentation\n",
       "2          interested\n",
       "3             welcome\n",
       "4              rainer\n",
       "...               ...\n",
       "198795      community\n",
       "198796          thing\n",
       "198797           fire\n",
       "198798        current\n",
       "198799           seek\n",
       "\n",
       "[198800 rows x 1 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fdb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57845792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e2787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cea79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d4333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de467f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0cdc68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb0809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd27c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bee1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e786d45",
   "metadata": {},
   "source": [
    "# 문장 임베딩 방식 중 \n",
    "\n",
    "## 1. mean pooling, max pooling\n",
    "## 2. SBERT\n",
    "\n",
    "## vs\n",
    "\n",
    "\n",
    "## 3. text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864da7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0c4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_df = copy.copy(news_df[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c83d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d54bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35901f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad821914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68a2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c3b512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df['embed'] = 0\n",
    "\n",
    "token_df['embed'] = token_df['embed'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e21fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model='text-embedding-ada-002'):\n",
    "    return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "for i in range(len(token_df)):\n",
    "    token_df['embed'][i]=get_embedding(token_df['token'][i], 'text-embedding-ada-002') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c52ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02f38bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.4134, 0.1037, 0.2178, 0.2650],\n",
       "         [0.2595, 0.1391, 0.4145, 0.1869],\n",
       "         [0.4292, 0.2928, 0.2549, 0.0231],\n",
       "         [0.3870, 0.4230, 0.1451, 0.0449],\n",
       "         [0.7312, 0.0540, 0.0489, 0.1660],\n",
       "         [0.2136, 0.1717, 0.3623, 0.2524],\n",
       "         [0.4050, 0.1230, 0.1802, 0.2918],\n",
       "         [0.4218, 0.1864, 0.2383, 0.1535],\n",
       "         [0.3142, 0.1323, 0.1852, 0.3683],\n",
       "         [0.3687, 0.2774, 0.1273, 0.2266],\n",
       "         [0.2961, 0.0409, 0.2684, 0.3946],\n",
       "         [0.3481, 0.3017, 0.1999, 0.1503]]),\n",
       " tensor([[0.2526, 0.0102, 0.2944, 0.4428],\n",
       "         [0.4815, 0.2458, 0.1800, 0.0927],\n",
       "         [0.5910, 0.0948, 0.1907, 0.1235],\n",
       "         [0.8810, 0.0377, 0.0418, 0.0394],\n",
       "         [0.5698, 0.2154, 0.1311, 0.0836],\n",
       "         [0.3289, 0.3396, 0.1686, 0.1629],\n",
       "         [0.2627, 0.2477, 0.2413, 0.2483],\n",
       "         [0.7987, 0.1050, 0.0350, 0.0613],\n",
       "         [0.7698, 0.0826, 0.0497, 0.0979],\n",
       "         [0.2519, 0.0202, 0.3724, 0.3555],\n",
       "         [0.5892, 0.2489, 0.0771, 0.0848],\n",
       "         [0.7112, 0.0632, 0.0995, 0.1261]]),\n",
       " tensor([[0.5001, 0.0959, 0.2181, 0.1858],\n",
       "         [0.2961, 0.2703, 0.2837, 0.1500],\n",
       "         [0.5976, 0.1278, 0.1530, 0.1215],\n",
       "         [0.3088, 0.2261, 0.2411, 0.2240],\n",
       "         [0.4151, 0.1287, 0.1797, 0.2764],\n",
       "         [0.2959, 0.2480, 0.2326, 0.2235],\n",
       "         [0.2562, 0.2903, 0.2312, 0.2223],\n",
       "         [0.2718, 0.2906, 0.2748, 0.1628],\n",
       "         [0.3359, 0.1218, 0.3158, 0.2265],\n",
       "         [0.4008, 0.1878, 0.2167, 0.1947],\n",
       "         [0.2768, 0.4314, 0.1284, 0.1634],\n",
       "         [0.2670, 0.0450, 0.3337, 0.3543]]),\n",
       " tensor([[0.1644, 0.4335, 0.2642, 0.1378],\n",
       "         [0.2885, 0.0982, 0.4051, 0.2082],\n",
       "         [0.2682, 0.0731, 0.3545, 0.3042],\n",
       "         [0.2815, 0.3327, 0.1548, 0.2310],\n",
       "         [0.2582, 0.3381, 0.2544, 0.1493],\n",
       "         [0.3241, 0.1575, 0.2999, 0.2185],\n",
       "         [0.2620, 0.0764, 0.3161, 0.3454],\n",
       "         [0.2870, 0.3616, 0.1413, 0.2102],\n",
       "         [0.3014, 0.3320, 0.2259, 0.1407],\n",
       "         [0.2470, 0.3539, 0.2280, 0.1711],\n",
       "         [0.3516, 0.0816, 0.4226, 0.1442],\n",
       "         [0.2754, 0.3323, 0.1377, 0.2546]]),\n",
       " tensor([[0.2697, 0.2268, 0.2709, 0.2327],\n",
       "         [0.2729, 0.1825, 0.4001, 0.1446],\n",
       "         [0.2907, 0.1719, 0.1866, 0.3508],\n",
       "         [0.2410, 0.2798, 0.2343, 0.2448],\n",
       "         [0.2498, 0.1398, 0.1503, 0.4601],\n",
       "         [0.2466, 0.1286, 0.4092, 0.2156],\n",
       "         [0.2534, 0.3054, 0.1978, 0.2433],\n",
       "         [0.2619, 0.1256, 0.3733, 0.2392],\n",
       "         [0.2806, 0.2276, 0.2398, 0.2519],\n",
       "         [0.2653, 0.3504, 0.1753, 0.2090],\n",
       "         [0.3045, 0.1815, 0.2967, 0.2173],\n",
       "         [0.3142, 0.1735, 0.2224, 0.2898]]),\n",
       " tensor([[9.9338e-01, 6.2814e-04, 2.0560e-03, 3.9367e-03],\n",
       "         [9.9491e-01, 5.7546e-04, 8.3966e-04, 3.6707e-03],\n",
       "         [9.8729e-01, 1.6944e-03, 2.9436e-03, 8.0734e-03],\n",
       "         [9.8345e-01, 1.9320e-03, 3.6025e-03, 1.1013e-02],\n",
       "         [9.5776e-01, 1.0718e-02, 9.2489e-03, 2.2271e-02],\n",
       "         [8.8834e-01, 1.7737e-02, 3.7356e-02, 5.6567e-02],\n",
       "         [9.8566e-01, 2.0129e-03, 2.4047e-03, 9.9219e-03],\n",
       "         [9.9029e-01, 1.0284e-03, 5.0158e-03, 3.6638e-03],\n",
       "         [3.9216e-01, 2.4761e-01, 2.1843e-01, 1.4180e-01],\n",
       "         [9.7309e-01, 4.6483e-03, 3.6719e-03, 1.8593e-02],\n",
       "         [9.8785e-01, 1.4743e-03, 3.2557e-03, 7.4211e-03],\n",
       "         [9.7966e-01, 2.9306e-03, 1.2190e-02, 5.2210e-03]])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e27de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8640266",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_token =pd.read_csv('use_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "749c70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_tree= pd.read_csv('layer1_tree.csv')\n",
    "layer2_tree= pd.read_csv('layer2_tree.csv')\n",
    "layer3_tree= pd.read_csv('layer3_tree.csv')\n",
    "layer4_tree= pd.read_csv('layer4_tree.csv')\n",
    "layer5_tree= pd.read_csv('layer5_tree.csv')\n",
    "layer6_tree= pd.read_csv('layer6_tree.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83189357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18250c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db793c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d0605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
